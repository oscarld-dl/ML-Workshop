{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oscarld-dl/ML-Workshop/blob/main/Predictive_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c3e2bc0-b597-4eec-af29-87dd27758649",
      "metadata": {
        "id": "4c3e2bc0-b597-4eec-af29-87dd27758649"
      },
      "source": [
        "<img src=\"https://github.com/oscarld-dl/ML-Workshop/blob/main/head.jpg?raw=1\" alt=\"Header Image\" width=\"1200\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "818899ba-a537-4c5c-b842-84c322fa67d2",
      "metadata": {
        "id": "818899ba-a537-4c5c-b842-84c322fa67d2"
      },
      "source": [
        "# ML Workshop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57a9565f-3cde-48b9-b702-364c751e3338",
      "metadata": {
        "id": "57a9565f-3cde-48b9-b702-364c751e3338"
      },
      "source": [
        "RWTH Aachen University, 07.10.2025"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79386d9e-1eef-4887-a5ed-119bbc83f81b",
      "metadata": {
        "id": "79386d9e-1eef-4887-a5ed-119bbc83f81b"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "***************************************************************************************************************************\n",
        "**The goal of this workshop is to understand how we have developed the descriptive and predictive analyses using ML/DL tools.**\n",
        "***************************************************************************************************************************\n",
        "\n",
        "**Tasks for the Predictive Analysis:**\n",
        "-------------\n",
        "\n",
        "1. Understanding **K-Fold Cross-Validation**.\n",
        "2. Using **scikit.learn** to evaluate different **linear regression models**.\n",
        "3. Upgrading to **stacked regression models** to improve accuracy.\n",
        "4. Using the best performing model to **generate the outputs of new synthetic inputs** (after data augmentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e12459-668b-4bcb-92a5-42e86d01950f",
      "metadata": {
        "id": "e9e12459-668b-4bcb-92a5-42e86d01950f"
      },
      "source": [
        "**Import basic libraries again:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b7190ff-5fa7-40f0-931e-20644f085717",
      "metadata": {
        "id": "3b7190ff-5fa7-40f0-931e-20644f085717"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c01905a9-c75e-44dc-96fe-452f2c945a66",
      "metadata": {
        "id": "c01905a9-c75e-44dc-96fe-452f2c945a66"
      },
      "source": [
        "**Import specific libraries:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceedbccb-669e-4fec-ab4b-4dd613c84fbf",
      "metadata": {
        "id": "ceedbccb-669e-4fec-ab4b-4dd613c84fbf"
      },
      "source": [
        "[Visit scikit-learn webpage HERE for more information](https://scikit-learn.org/stable/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a2e6d03-039d-445d-a30f-e94b549f6960",
      "metadata": {
        "id": "5a2e6d03-039d-445d-a30f-e94b549f6960"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, RepeatedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
        "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, Lars, LassoLars, LassoLarsIC, BayesianRidge, MultiTaskLasso, MultiTaskLassoCV, HuberRegressor, ElasticNet, ElasticNetCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff7015c1-4492-4a4f-aae1-8e168d197e26",
      "metadata": {
        "id": "ff7015c1-4492-4a4f-aae1-8e168d197e26"
      },
      "source": [
        "**Understanding the Cross-Validation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d5ee10d-6744-476c-a00d-1a87a955a1be",
      "metadata": {
        "id": "7d5ee10d-6744-476c-a00d-1a87a955a1be"
      },
      "source": [
        "<img src=\"https://github.com/oscarld-dl/ML-Workshop/blob/main/cv.jpg?raw=1\" alt=\"Header Image\" width=\"1000\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd0ec46d-c1ef-4f70-bd14-511f81be490b",
      "metadata": {
        "id": "fd0ec46d-c1ef-4f70-bd14-511f81be490b"
      },
      "source": [
        "1. Dataset is split into K subsets.\n",
        "2. The algorithm is then iterated K (fold) times and in each fold different subsets are used as a validation set (green ones)\n",
        "3. This help us to determine the model's sensitivity to new and unseen data.\n",
        "4. Finally a holdout/test set is used to evaluate the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93c40bed-bba1-4111-adb4-f9c9ce34b547",
      "metadata": {
        "id": "93c40bed-bba1-4111-adb4-f9c9ce34b547"
      },
      "source": [
        "**Let's gather different models from the library**\n",
        "\n",
        "1. Why normalization.\n",
        "2. Why splitting the data.\n",
        "\n",
        "**Hands-on**\n",
        "\n",
        "1. Add base models. Take just the most important ones but GB at the beginning.\n",
        "2. Try different K folders to see how it takes more computational time to solve. Also to check how accuracy changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b6e7a2b-2dbb-4309-9191-f6cf678293f7",
      "metadata": {
        "id": "3b6e7a2b-2dbb-4309-9191-f6cf678293f7"
      },
      "outputs": [],
      "source": [
        "# Load the Excel file again\n",
        "df_experimental = pd.read_excel(\"D:/THESIS/OUTPUTS/master.xlsx\", sheet_name=\"experimental\")\n",
        "\n",
        "# Input and output split\n",
        "X = df_experimental.iloc[:89, 1:12]\n",
        "print(X)\n",
        "Y = df_experimental.iloc[:89, 12:]\n",
        "\n",
        "# Normalize inputs and outputs\n",
        "scaler_X = StandardScaler()\n",
        "scaler_Y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "Y_scaled = scaler_Y.fit_transform(Y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_scaled, test_size=18, random_state=42, shuffle=False)\n",
        "\n",
        "# Define models to test\n",
        "base_models = {\n",
        "    \"Random Forest\": RandomForestRegressor(),\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(),\n",
        "    \"AdaBoost\": AdaBoostRegressor(),\n",
        "    \"Decision Tree\": DecisionTreeRegressor(),\n",
        "    \"Ridge\": Ridge(),\n",
        "    \"RidgeCV\": RidgeCV(),\n",
        "    \"Lasso\": Lasso(),\n",
        "    \"LassoCV\": LassoCV(),\n",
        "    \"Lars\": Lars(),\n",
        "    \"ElasticNet\": ElasticNet(),\n",
        "    \"ElasticNetCV\": ElasticNetCV(),\n",
        "    \"LassoLars\": LassoLars(),\n",
        "    \"LassoLarsIC\": LassoLarsIC(),\n",
        "    \"BayesianRidge\": BayesianRidge(),\n",
        "    \"HuberRegressor\": HuberRegressor(),\n",
        "    \"SVR (RBF Kernel)\": SVR(kernel='rbf'),\n",
        "    \"MLP Regressor\": MLPRegressor(hidden_layer_sizes=(200, 100), activation='relu', solver='lbfgs', max_iter=3500, random_state=42)\n",
        "}\n",
        "\n",
        "\n",
        "results_per_model = {}\n",
        "rkf = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)  # Reduce repeats for speed if needed\n",
        "\n",
        "for name, model in base_models.items():\n",
        "    print(f\"Evaluating {name}...\")\n",
        "    multi_model = MultiOutputRegressor(model)\n",
        "    r2_scores = []\n",
        "\n",
        "    for train_idx, test_idx in rkf.split(X_scaled):\n",
        "        X_train, X_val = X_scaled[train_idx], X_scaled[test_idx]\n",
        "        Y_train, Y_val = Y_scaled[train_idx], Y_scaled[test_idx]\n",
        "\n",
        "        multi_model.fit(X_train, Y_train)\n",
        "        Y_pred = multi_model.predict(X_val)\n",
        "\n",
        "        fold_r2 = r2_score(Y_val, Y_pred, multioutput='raw_values')  # List of R² for each output\n",
        "        r2_scores.append(fold_r2)\n",
        "\n",
        "    # Average R² per output\n",
        "    r2_scores = np.array(r2_scores)\n",
        "    avg_r2_per_output = np.mean(r2_scores, axis=0)\n",
        "    results_per_model[name] = avg_r2_per_output\n",
        "\n",
        "results_df = pd.DataFrame(results_per_model, index=Y.columns).T  # Models as rows, outputs as columns\n",
        "results_df[\"Average R² Score\"] = results_df.mean(axis=1)\n",
        "\n",
        "# Sort\n",
        "results_df.sort_values(by=\"Average R² Score\", ascending=False, inplace=True)\n",
        "\n",
        "# Save\n",
        "#results_df.to_csv(\"D:/THESIS/OUTPUTS/model_performance_detailed_CV.csv\", index=True)\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a86d5115-b76c-4aaf-939a-95a21e8411bb",
      "metadata": {
        "id": "a86d5115-b76c-4aaf-939a-95a21e8411bb"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.metrics import r2_score, make_scorer\n",
        "\n",
        "def multioutput_r2_score(y_true, y_pred):\n",
        "    return np.mean([r2_score(y_true[:, i], y_pred[:, i]) for i in range(y_true.shape[1])])\n",
        "\n",
        "scorer = make_scorer(multioutput_r2_score, greater_is_better=True)\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_excel(\"D:/THESIS/OUTPUTS/master.xlsx\", sheet_name=\"experimental\")\n",
        "\n",
        "# Input and output split\n",
        "X = df.iloc[1:89, 1:12]\n",
        "Y = df.iloc[1:89, 12:]\n",
        "\n",
        "# Normalize inputs and outputs\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y_scaled = scaler.fit_transform(Y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_scaled, test_size=18, random_state=0)\n",
        "\n",
        "# Define candidate base models\n",
        "candidate_models = {\n",
        "    'MLP Regressor': MLPRegressor(hidden_layer_sizes=(200, 100), activation='relu', solver='lbfgs', max_iter=3550, random_state=0),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(),\n",
        "    'RF': RandomForestRegressor()\n",
        "}\n",
        "\n",
        "meta_model = SVR()\n",
        "\n",
        "#Store results\n",
        "stacking_results = []\n",
        "rkf = RepeatedKFold(n_splits=5, n_repeats=100, random_state=42)\n",
        "\n",
        "# Try all pairwise combinations of base models\n",
        "for combo in combinations(candidate_models.items(), 2):\n",
        "    name_combo = ' + '.join([combo[0][0], combo[1][0]])\n",
        "    base_learners = [(combo[0][0], combo[0][1]), (combo[1][0], combo[1][1])]\n",
        "\n",
        "    stacked_model = StackingRegressor(estimators=base_learners, final_estimator=meta_model)\n",
        "    multi_model = MultiOutputRegressor(stacked_model)\n",
        "    scores = cross_val_score(multi_model, X_scaled, Y_scaled, cv=rkf, scoring=scorer)\n",
        "    avg_score = np.mean(scores)\n",
        "    stacking_results.append((name_combo, avg_score))\n",
        "\n",
        "# Print sorted results\n",
        "stacking_results_df = pd.DataFrame(stacking_results, columns=[\"Model\", \"Average R² Score\"])\n",
        "stacking_results_df.sort_values(by=\"Average R² Score\", ascending=False, inplace=True)\n",
        "print(stacking_results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ee9f3e2-60cb-414b-a487-c70cdad3b6e7",
      "metadata": {
        "id": "4ee9f3e2-60cb-414b-a487-c70cdad3b6e7"
      },
      "source": [
        "**Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36b3a1e7-55a1-413c-8e43-607e0b1379ff",
      "metadata": {
        "id": "36b3a1e7-55a1-413c-8e43-607e0b1379ff"
      },
      "outputs": [],
      "source": [
        "#1000 DESIGNS\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "\n",
        "output_file = r\"D:\\THESIS\\OUTPUTS\\Generator.xlsx\"\n",
        "num_designs = 20000 # Target number of random combinations\n",
        "\n",
        "# Define synthetic value ranges for each parameter\n",
        "synthetic_values = {\n",
        "    \"C\": [0.0014, 0.0015, 0.0016, 0.0017, 0.0018, 0.0019, 0.0020],\n",
        "    \"$H_{T1}$\": [880, 890, 900, 910, 920, 930, 940],\n",
        "    \"$H_{T2}$\": [790, 800, 810, 820, 830, 840, 850],\n",
        "    \"P\": [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    \"d\": [0.0335, 0.0435, 0.0535, 0.0635, 0.0735, 0.0835, 0.0935],\n",
        "    \"alpha\": [0.5, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90],\n",
        "    \"Pearlite\": [0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50],\n",
        "    \"$Cryo_{T}$\": [-75, -80, -90, -100, -110, -120, -125],\n",
        "    \"$Cryo_{t}$\": [2400, 2800, 3200, 3600, 3900, 4500, 4800],\n",
        "    \"$Temper_{T}$\": [150, 160, 170, 185, 200, 225, 250],\n",
        "    \"Temper_{t}\": [3600, 4200, 4800, 5400, 6000, 6600, 7200]\n",
        "}\n",
        "\n",
        "# Generate random combinations\n",
        "random_combinations = []\n",
        "for _ in range(num_designs):\n",
        "    combination = {param: random.choice(values) for param, values in synthetic_values.items()}\n",
        "    if combination[\"alpha\"] + combination[\"Pearlite\"] == 1:\n",
        "            random_combinations.append(combination)\n",
        "\n",
        "# Create DataFrame\n",
        "synthetic_df = pd.DataFrame(random_combinations)\n",
        "\n",
        "# Append to existing file or create new\n",
        "if os.path.exists(output_file):\n",
        "    existing_df = pd.read_excel(output_file)\n",
        "    df_combined = pd.concat([existing_df, synthetic_df], ignore_index=True)\n",
        "else:\n",
        "    df_combined = synthetic_df\n",
        "\n",
        "df_combined.to_excel(output_file, index=False, engine='openpyxl')\n",
        "print(f\"Generated {len(synthetic_df)} random designs. Data saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23bc13d2-5ecf-4ad6-b76d-154f50843116",
      "metadata": {
        "id": "23bc13d2-5ecf-4ad6-b76d-154f50843116"
      },
      "source": [
        "**GB model as the output generator for synthetic input data.**\n",
        "\n",
        "1. Explain here the other way to split the data using **train_test_split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0ef721b-6534-4a9e-bec2-390ad7316809",
      "metadata": {
        "id": "c0ef721b-6534-4a9e-bec2-390ad7316809"
      },
      "outputs": [],
      "source": [
        "# Loading real data for training\n",
        "df_real = pd.read_excel(\"D:/THESIS/OUTPUTS/master.xlsx\", sheet_name=\"experimental\")\n",
        "X= df_real.iloc[:88, 1:12]\n",
        "Y= df_real.iloc[:88, 12:]\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_Y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "Y_scaled = scaler_Y.fit_transform(Y)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_scaled, test_size=18, random_state=42, shuffle=False)\n",
        "\n",
        "\n",
        "# Loading synthetic data for the 1000 predicted outputs\n",
        "df_synthetic_1000=pd.read_excel(\"D:/THESIS/OUTPUTS/DoEs_1000.xlsx\").iloc[:1002, :11]\n",
        "X_synth_1000 = df_synthetic_1000.copy()\n",
        "X_synth_1000_scaled=scaler_X.transform(X_synth_1000)\n",
        "\n",
        "# Loading synthetic data for the 10000 predicted outputs\n",
        "df_synthetic_10000=pd.read_excel(\"D:/THESIS/OUTPUTS/DoEs_10000.xlsx\").iloc[:10002, :11]\n",
        "X_synth_10000 = df_synthetic_10000.copy()\n",
        "X_synth_10000_scaled=scaler_X.transform(X_synth_10000)\n",
        "\n",
        "ensemble_preds = []\n",
        "ensemble_preds_1000_synthetic=[]\n",
        "ensemble_preds_10000_synthetic=[]\n",
        "train_r2_scores=[]\n",
        "test_r2_scores=[]\n",
        "\n",
        "for i in range(42, 56):\n",
        "    base_gb = GradientBoostingRegressor(random_state=i)\n",
        "    gb = MultiOutputRegressor(base_gb)\n",
        "    gb.fit(X_train, Y_train)\n",
        "\n",
        "    preds = gb.predict(X_scaled)\n",
        "    ensemble_preds.append(preds)\n",
        "\n",
        "    preds_1000_synthetic=gb.predict(X_synth_1000_scaled)\n",
        "    ensemble_preds_1000_synthetic.append(preds_1000_synthetic)\n",
        "\n",
        "    preds_10000_synthetic=gb.predict(X_synth_10000_scaled)\n",
        "    ensemble_preds_10000_synthetic.append(preds_10000_synthetic)\n",
        "\n",
        "    train_preds=gb.predict(X_train)\n",
        "    test_preds=gb.predict(X_test)\n",
        "    # print(test_preds.shape)\n",
        "\n",
        "    train_r2=r2_score(Y_train, train_preds, multioutput='raw_values')\n",
        "    train_r2_scores.append(train_r2)\n",
        "\n",
        "    test_r2=r2_score(Y_test, test_preds, multioutput='raw_values')\n",
        "    test_r2_scores.append(test_r2)\n",
        "\n",
        "train_r2_scores=np.array(train_r2_scores)\n",
        "test_r2_scores=np.array(test_r2_scores)\n",
        "\n",
        "# Calculate mean (14 models, one for each random state=i) predictions for NOT synthetic.\n",
        "ensemble_preds = np.stack(ensemble_preds)\n",
        "Y_not_synth_mean = np.mean(ensemble_preds, axis=0)\n",
        "Y_not_synth_physical = scaler_Y.inverse_transform(Y_not_synth_mean)\n",
        "\n",
        "# Calculate mean (14 models, one for each random state=i) predictions for synthethic.\n",
        "ensemble_preds_1000_synthetic=np.stack(ensemble_preds_1000_synthetic)\n",
        "Y_1000_synth_mean=np.mean(ensemble_preds_1000_synthetic, axis=0)\n",
        "Y_1000_synth_physical = scaler_Y.inverse_transform(Y_synth_mean)\n",
        "\n",
        "# Calculate mean (14 models, one for each random state=i) predictions for synthethic.\n",
        "ensemble_preds_10000_synthetic=np.stack(ensemble_preds_10000_synthetic)\n",
        "Y_10000_synth_mean=np.mean(ensemble_preds_10000_synthetic, axis=0)\n",
        "Y_10000_synth_physical = scaler_Y.inverse_transform(Y_10000_synth_mean)\n",
        "\n",
        "\n",
        "# Convertir a DataFrame\n",
        "output_columns = df_real.columns[12:]\n",
        "\n",
        "avg_train_r2 = np.mean(train_r2_scores, axis=0)\n",
        "avg_test_r2 = np.mean(test_r2_scores, axis=0)\n",
        "std_train_r2 = np.std(train_r2_scores, axis=0)\n",
        "std_test_r2 = np.std(test_r2_scores, axis=0)\n",
        "\n",
        "Y_not_aug_df = pd.DataFrame(Y_not_synth_physical, columns=output_columns)\n",
        "not_synthetic_full_df = pd.concat([X_not_synth.reset_index(drop=True), Y_not_aug_df], axis=1)\n",
        "\n",
        "Y_1000_df=pd.DataFrame(Y_1000_synth_physical, columns=output_columns)\n",
        "synthetic_1000_full_df=pd.concat([X_synth_1000.reset_index(drop=True), Y_1000_df], axis=1)\n",
        "\n",
        "Y_10000_df=pd.DataFrame(Y_10000_synth_physical, columns=output_columns)\n",
        "synthetic_10000_full_df=pd.concat([X_synth_10000.reset_index(drop=True), Y_10000_df], axis=1)\n",
        "\n",
        "results_not_df = pd.DataFrame({\n",
        "    'Output': output_columns,\n",
        "    'Average Train R²': avg_train_r2,\n",
        "    'Std Train R²': std_train_r2,\n",
        "    'Average Test R²': avg_test_r2,\n",
        "    'Std Test R²': std_test_r2\n",
        "})\n",
        "\n",
        "\n",
        "print(\"\\nPerformance per Output Variable:\")\n",
        "print(results_not_df.round(4))\n",
        "\n",
        "# Guardar en la hoja \"synthetic\"\n",
        "with pd.ExcelWriter(\"D:/THESIS/OUTPUTS/master.xlsx\", engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
        "    #not_synthetic_full_df.to_excel(writer, sheet_name=\"train_test\", index=False) #this line writes the results of 86d original data GB outputs\n",
        "    #results_not_df.to_excel(writer, sheet_name=\"train_test_R2\", index=False) #this line writes the R2 and SD results from 86d predicted outputs\n",
        "    #synthetic_1000_full_df.to_excel(writer, sheet_name=\"1000_synthetic\", index=False) #this line writes the GB 1000 designs outputs\n",
        "    synthetic_10000_full_df.to_excel(writer, sheet_name=\"10000_synthetic\", index=False) #this line writes the GB 10000 designs outputs\n",
        "\n",
        "print(\"Saved successfully!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 (tensorflow)",
      "language": "python",
      "name": "python310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}